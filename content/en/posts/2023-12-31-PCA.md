---
title: "PCA:Linear Algebra revisited"
date: 2022-12-31
author: "Fangyuan Sheng"
slug: la
draft: false
toc: true
---
{{<block class="info">}}
This blog aims to help you understand PCA geometrically. Many thanks to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra). 
{{<end>}}

## Concept 1: Linear Transformation

To understand PCA, we need to know something about linear algebra. 

Linear transformation is a transformation that

(1) Keeps lines to be lines after being transformd

(2) With Origin fixed

So these transformations are not linear.

{{<figure src="https://hellenshengfy.github.io/PCA1.JPG" title="Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)">}}

{{<figure src="https://hellenshengfy.github.io/PCA2.JPG" title="Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)">}}

## Concept 2: Matrix

You can simpley think of a matrix as a box of vectors. Otherwise, you could think of it as a linear transformation.

For example, `$$\begin{bmatrix} k & 0 \\ 0 & 1 \end{bmatrix}$$` is a scaler matrix which scales the x-axis by k.

{{<figure src="https://hellenshengfy.github.io/PCA3.jpg" title="Photo credit to [Visual Kernel](https://youtu.be/7Gtxd-ew4lk?si=_8UlBQHYpOfHfyDa)">}}


## Concept 3: Eigenvectors and Singular Vectors

Eigenvectors are vectors of invariant action. Intuitively, it is the axis of a transformation. Not all matrix have eigenvectors.

{{<figure src="https://hellenshengfy.github.io/PCA4.jpg" title="Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)">}}

{{<figure src="https://hellenshengfy.github.io/PCA5.jpg" title="Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)">}}

Singular vecotrs are vectors of maximum action[1]. All matrix have singular vectors.


## Concept 4: Covariance Matrix

Covariance matrix records of data correlation of a data set.

{{<figure src="https://hellenshengfy.github.io/PCA6.jpg" title="Photo credit to [Serrano.Academy](https://youtu.be/WBlnwvjfMtQ?si=eNxoUaErt4V0L5q7)">}}

Mathmatically, it can be calculated as:

 `$C=\frac{1}{n-1}MM^{T}$`
 
{{<figure src="https://hellenshengfy.github.io/PCA7.jpg" title="Photo credit to https://b23.tv/O1k2tQK">}}


## Concept 5: PCA

PCA is most commonly used to reduce data dimension by projecting data set to the direction of the max variance.

So, how to find principle components of a data set?

You can either:

(1) Find the eigenvectors of the covariance matrix of the data. Eigenvectors are principle components!

{{<figure src="https://hellenshengfy.github.io/PCA8.jpg" title="Photo credit to https://b23.tv/O1k2tQK">}}

{{<figure src="https://hellenshengfy.github.io/PCA9.jpg" title="Photo credit to https://b23.tv/O1k2tQK">}}

{{<figure src="https://hellenshengfy.github.io/PCA10.jpg" title="Photo credit to https://b23.tv/O1k2tQK">}}

{{<figure src="https://hellenshengfy.github.io/PCA11.jpg" title="Photo credit to https://b23.tv/O1k2tQK">}}

or:

(2) Perform SDV on the data set. Vectors within matrix V are principle components!

Why these two methods give us the same solution? Here is why:

{{<figure src="https://hellenshengfy.github.io/PCA12.jpg" title="Photo credit to https://b23.tv/O1k2tQK">}}


Reference:

[1] https://zhuanlan.zhihu.com/p/353637184?utm_id=0

[2] https://www.3blue1brown.com/topics/linear-algebra

[3] https://youtu.be/7Gtxd-ew4lk?si=_8UlBQHYpOfHfyDa

[4] https://b23.tv/sc4Cbp0

[5] https://b23.tv/O1k2tQK

[6] https://youtu.be/WBlnwvjfMtQ?si=eNxoUaErt4V0L5q7
