---
title: "PCA:Linear Algebra revisited"
date: 2022-12-31
author: "Fangyuan Sheng"
slug: la
draft: false
toc: true
---
{{<block class="info">}}
This blog aims to help you understand PCA geometrically. Many thanks to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra). 
{{<end>}}

## Concept 1: Linear Transformation

To understand PCA, we need to know something about linear algebra. 

Linear transformation is a transformation that

(1) Keeps lines to be lines after being transformd

(2) With Origin fixed

So these transformations are not linear.

图片

## Concept 2: Matrix

You can simpley think of a matrix as a box of vectors. Otherwise, you could think of it as a linear transformation.

For example, `$$\begin{bmatrix} k & 0 \\ 0 & 1 \end{bmatrix}$$` is a scaler matrix which scales the x-axis by k.

图片

## Concept 3: Eigenvectors and Singular Vectors

Eigenvectors are vectors of invariant action. Intuitively, it is the axis of a transformation. Not all matrix have eigenvectors.

Singular vecotrs are vectors of maximum action. All metrix have singular vectors.

图片


## Concept 4: Covariance Matrix

Covariance matrix records of data correlation of a data set.

Mathmatically, it can be calculated as:


图片
## Concept 5: PCA

PCA is most commonly used to reduce data dimension by projecting data set to the direction of the max variance.

So, how to find principle components of a data set?

You can either:

(1) Find the eigenvectors of the covariance matrix of the data. Eigenvectors are principle components!

or:

(2) Perform SDV on the data set. Vectors within matrix V are principle components!

Why these two methods give us the same solution? Here is why:

图片

