<a name=top></a><!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Hugo-HT</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><link href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/bash.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js></script><script src=https://cdn.jsdelivr.net/npm/vega@5.17.0></script><script src=https://cdn.jsdelivr.net/npm/vega-lite@4.17.0></script><script src=https://cdn.jsdelivr.net/npm/vega-embed@6.12.2></script><script>hljs.initHighlightingOnLoad();</script><link rel=icon href=https://hellenshengfy.github.io/media/hugo-logo.png></head><body><div class=wrapper><header class=header><nav class=nav><a href=/ class=nav-logo><img src=/media/hugo-logo.png width=50 height=50 alt=Hugo-ht></a><ul class=nav-links><li><a href=/>Home</a></li><li><a href=/en/about/>About</a></li><li><a href=/en/notes/>My projects</a></li><li><a href=/en/posts/>Blog</a></li><li><a href=/cn/posts/>中文</a></li></ul></nav></header><main class=content role=main><div style=text-align:center><h1>PCA:Linear Algebra revisited</h1><p>Fangyuan Sheng
/ 2022-12-31</p><hr></div><span class=article-toolbar><a href=https://github.com/hellenshengfy/hellenshengfy.github.io/edit/master/content/en/posts/2023-12-31-PCA.md style=font-size:24px;color:#000 target=_blank><i class="fa fa-edit" aria-hidden=true title="Suggest an edit of this page"></i></a></span><aside class=toc>Table of Contents:<nav id=TableOfContents><ol><li><a href=#concept-1-linear-transformation>Concept 1: Linear Transformation</a></li><li><a href=#concept-2-matrix>Concept 2: Matrix</a></li><li><a href=#concept-3-eigenvectors-and-singular-vectors>Concept 3: Eigenvectors and Singular Vectors</a></li><li><a href=#concept-4-covariance-matrix>Concept 4: Covariance Matrix</a></li><li><a href=#concept-5-pca>Concept 5: PCA</a></li></ol></nav></aside><div class="body-text list-text"><p><div class=info>This blog aims to help you understand PCA geometrically. Many thanks to <a href=https://www.3blue1brown.com/topics/linear-algebra target=_blank rel="noreferrer noopener">3blue1brown&rsquo;s videos: esscence of linear algebra</a>
.</div></p><h2 id=concept-1-linear-transformation>Concept 1: Linear Transformation<a href=#concept-1-linear-transformation class=header-anchor arialabel=Anchor> #</a></h2><p>To understand PCA, we need to know something about linear algebra.</p><p>Linear transformation is a transformation that</p><p>(1) Keeps lines to be lines after being transformd</p><p>(2) With Origin fixed</p><p>So these transformations are not linear.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA1.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA1.jpg></a><figcaption><h4>Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA2.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA2.jpg></a><figcaption><h4>Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)</h4></figcaption></figure><h2 id=concept-2-matrix>Concept 2: Matrix<a href=#concept-2-matrix class=header-anchor arialabel=Anchor> #</a></h2><p>You can simpley think of a matrix as a box of vectors. Otherwise, you could think of it as a linear transformation.</p><p>For example, <code>$$\begin{bmatrix} k & 0 \\ 0 & 1 \end{bmatrix}$$</code> is a scaler matrix which scales the x-axis by k.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA3.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA3.jpg></a><figcaption><h4>Photo credit to [Visual Kernel](https://youtu.be/7Gtxd-ew4lk?si=_8UlBQHYpOfHfyDa)</h4></figcaption></figure><h2 id=concept-3-eigenvectors-and-singular-vectors>Concept 3: Eigenvectors and Singular Vectors<a href=#concept-3-eigenvectors-and-singular-vectors class=header-anchor arialabel=Anchor> #</a></h2><p>Eigenvectors are vectors of invariant action. Intuitively, it is the axis of a transformation. Not all matrix have eigenvectors.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA4.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA4.jpg></a><figcaption><h4>Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA5.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA5.jpg></a><figcaption><h4>Photo credit to [3blue1brown's videos: esscence of linear algebra](https://www.3blue1brown.com/topics/linear-algebra)</h4></figcaption></figure><p>Singular vecotrs are vectors of maximum action[1]. All metrix have singular vectors.</p><h2 id=concept-4-covariance-matrix>Concept 4: Covariance Matrix<a href=#concept-4-covariance-matrix class=header-anchor arialabel=Anchor> #</a></h2><p>Covariance matrix records of data correlation of a data set.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA6.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA6.jpg></a><figcaption><h4>Photo credit to [Serrano.Academy](https://youtu.be/WBlnwvjfMtQ?si=eNxoUaErt4V0L5q7)</h4></figcaption></figure><p>Mathmatically, it can be calculated as:</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA7.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA7.jpg></a><figcaption><h4>Photo credit to https://b23.tv/O1k2tQK</h4></figcaption></figure><h2 id=concept-5-pca>Concept 5: PCA<a href=#concept-5-pca class=header-anchor arialabel=Anchor> #</a></h2><p>PCA is most commonly used to reduce data dimension by projecting data set to the direction of the max variance.</p><p>So, how to find principle components of a data set?</p><p>You can either:</p><p>(1) Find the eigenvectors of the covariance matrix of the data. Eigenvectors are principle components!</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA8.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA8.jpg></a><figcaption><h4>Photo credit to https://b23.tv/O1k2tQK</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA9.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA9.jpg></a><figcaption><h4>Photo credit to https://b23.tv/O1k2tQK</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA10.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA10.jpg></a><figcaption><h4>Photo credit to https://b23.tv/O1k2tQK</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA11.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA11.jpg></a><figcaption><h4>Photo credit to https://b23.tv/O1k2tQK</h4></figcaption></figure><p>or:</p><p>(2) Perform SDV on the data set. Vectors within matrix V are principle components!</p><p>Why these two methods give us the same solution? Here is why:</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://hellenshengfy.github.io/PCA12.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://hellenshengfy.github.io/PCA12.jpg></a><figcaption><h4>Photo credit to https://b23.tv/O1k2tQK</h4></figcaption></figure><p>Reference:</p><p>[1] https://zhuanlan.zhihu.com/p/353637184?utm_id=0</p><p>[2] https://www.3blue1brown.com/topics/linear-algebra</p><p>[3] https://youtu.be/7Gtxd-ew4lk?si=_8UlBQHYpOfHfyDa</p><p>[4] https://b23.tv/sc4Cbp0</p><p>[5] https://b23.tv/O1k2tQK</p><p>[6] https://youtu.be/WBlnwvjfMtQ?si=eNxoUaErt4V0L5q7</p><p style=color:#777>Last modified on 2022-12-31</p></div><a href=#top><i class="fa fa-chevron-up" style=font-size:30px;color:#000></i></a></main><footer class=footer><script type=text/javascript src=/js/math-code.js></script><script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/javascript src=/js/center-img.js></script><ul class=footer-links><li><a href=/en/posts/index.xml type=application/rss+xml title="RSS feed">Subscribe</a></li><li><a href=http://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>License
<i class="fa fa-cc" aria-hidden=true title="Attribution-NonCommercial-ShareAlike 4.0 International"></i></a></li></ul><div class=copyright-text>©
Sheng Fangyuan
2020-2021</div></footer>